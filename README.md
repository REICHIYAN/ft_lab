
# TinyLlama Fine-Tuning Toolkit (Updated â€” No vLLM)

---

## ðŸŒ Overview

This repository provides a **compact, end-to-end fine-tuning and evaluation toolkit**
for **TinyLlama-1.1B-Chat-v1.0**, enabling reproducible experiments across:

- Full Fine-Tuning (FT)
- LoRA
- QLoRA
- RAG evaluation (HuggingFace embeddings via LlamaIndex)
- Unified model comparison utilities

vLLM is **not used in this project** and all related descriptions have been removed.

Prefix Tuning is intentionally excluded to keep the stack minimal.

---

## ðŸ—ï¸ Architecture & Tech Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Training Layer             â”‚
â”‚  â€¢ Full FT (HF Trainer)                   â”‚
â”‚  â€¢ LoRA (PEFT)                            â”‚
â”‚  â€¢ QLoRA (4bit + LoRA)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Model Outputs               â”‚
â”‚  models/ft_full/                          â”‚
â”‚  models/ft_lora/                          â”‚
â”‚  models/ft_qlora/                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Evaluation Utilities           â”‚
â”‚  â€¢ app_rag_compare.py (RAG pipeline)      â”‚
â”‚  â€¢ compare_adapters.py                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“š Fine-Tuning Methods

### **1. Full Fine-Tuning**

```bash
python train_full.py
```

---

### **2. LoRA**

```bash
python train_lora.py
```

---

### **3. QLoRA**

```bash
python train_qlora.py
```

---

## ðŸ” RAG Evaluation (Updated)

This project includes a simple RAG demo using **LlamaIndex** and **HuggingFace embeddings**.

We use:

- `llama-index-embeddings-huggingface`
- `sentence-transformers`

Example (inside `app_rag_compare.py`):

```python
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

embed_model = HuggingFaceEmbedding(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
```

Run:

```bash
python app_rag_compare.py --docs_dir docs --question "Explain LoRA."
```

---

## ðŸ§­ Model Comparison

```bash
python compare_adapters.py
```

Compares:

- Full FT
- LoRA
- QLoRA

---

## ðŸ“ Repository Structure

```
llm_ft_tinyllama/
â”œâ”€â”€ train_full.py
â”œâ”€â”€ train_lora.py
â”œâ”€â”€ train_qlora.py
â”œâ”€â”€ compare_adapters.py
â”œâ”€â”€ app_rag_compare.py
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ft_full/
â”‚   â”œâ”€â”€ ft_lora/
â”‚   â””â”€â”€ ft_qlora/
â”‚
â”œâ”€â”€ docs/
â”‚
â””â”€â”€ data/
    â””â”€â”€ toy_qa.jsonl
```

---

## ðŸ›  Requirements

The following are required:

```
torch>=2.1.0
transformers>=4.39.0
accelerate>=0.27.0
sentencepiece>=0.1.99
einops>=0.7.0

datasets>=2.18.0
peft>=0.10.0
bitsandbytes>=0.42.0

langchain>=0.2.0
langchain-openai>=0.1.0
llama-index>=0.10.0

python-dotenv>=1.0.0

llama-index-embeddings-huggingface
sentence-transformers
```

Install:

```bash
pip install -r requirements.txt
```

---

## ðŸ™Œ Final Notes

This repository is a **clean, extensible baseline** for TinyLlama fineâ€‘tuning and LlamaIndexâ€‘based RAG experiments.
