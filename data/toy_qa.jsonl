{"question": "What is TinyLlama?", "answer": "TinyLlama is a 1.1B-parameter causal language model designed as a compact LLaMA-style model."}
{"question": "How can TinyLlama be fine-tuned?", "answer": "You can fine-tune TinyLlama using full fine-tuning, LoRA, QLoRA, or prefix tuning. This project provides working minimal examples for each."}
{"question": "What is LoRA?", "answer": "LoRA is a parameter-efficient fine-tuning method that injects low-rank adapters into linear layers while keeping pretrained weights frozen."}
{"question": "What is QLoRA?", "answer": "QLoRA combines 4-bit quantization with LoRA adapters to minimize GPU memory usage during fine-tuning."}
{"question": "What is the purpose of this repo?", "answer": "It provides a clean TinyLlama playground for comparing full fine-tuning, LoRA, QLoRA, and prefix tuning using a small dataset."}
